"""
å¤§äº”äººæ ¼å›¾ç¥ç»ç½‘ç»œæ¨¡å‹ - ä¸»å…¥å£
ä½¿ç”¨PyG DataLoader + BERTåµŒå…¥
"""

import os
import argparse
import statistics
import torch
from datetime import datetime
from torch.utils.data import Subset
from torch.utils.data import WeightedRandomSampler
from torch_geometric.loader import DataLoader

os.environ["TOKENIZERS_PARALLELISM"] = "false"

from modules import (
    BigFiveDataset,
    BigFiveGNN,
    TrainingLogger,
    train_model,
    evaluate_on_test_set,
    _compute_scene_weights_for_subset
)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--focus_trait', type=str, default=None)
    args = parser.parse_args()

    def _map_trait(trait):
        if trait is None:
            return None
        names_cn = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§']
        names_en = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        try:
            idx = int(trait)
            if 0 <= idx < 5:
                return idx
        except Exception:
            pass
        if trait in names_cn:
            return names_cn.index(trait)
        t = str(trait).lower()
        if t in names_en:
            return names_en.index(t)
        return None

    focus_trait = _map_trait(args.focus_trait)

    config = {
        'data_path': "data/multilabel_dataset.json",
        'model_path': "best_bigfive.pth",
        'use_bert': True,
        'batch_size': 1,
        'epochs': 50,
        'learning_rate': 2e-5,
        'hidden_dim': 256,
        'val_size': 0.15,
        'test_size': 0.15,
        'random_seed': 42,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'use_weighted_scene_sampler': False,
        'scene_weight_alpha': 0.5,
        'scene_weight_clip_min': 1.0,
        'scene_weight_clip_max': 3.0,
        'samples_per_epoch_factor': 1.0,
        'focus_trait': focus_trait
    }

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name = "enhanced"
    log_filename = f"training_{model_name}_{timestamp}.log"

    if config['focus_trait'] is not None:
        trait_name = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§'][config['focus_trait']]
        log_filename = f"training_{model_name}_{trait_name}_{timestamp}.log"

    training_logger = TrainingLogger(log_dir="logs", log_filename=log_filename)
    training_logger.replace_print()

    model_info = {
        "æ¨¡å‹ç±»å‹": "å¤§äº”äººæ ¼å›¾ç¥ç»ç½‘ç»œæ¨¡å‹",
        "BERTåµŒå…¥": "å¯ç”¨" if config['use_bert'] else "ç¦ç”¨",
        "å¢å¼ºåˆ†ç±»å™¨": "å¯ç”¨",
        "è¯æ®å¥æŸå¤±æƒé‡": "0.8 (ä¼˜å…ˆä¼˜åŒ–è¯æ®å¥é¢„æµ‹)"
    }

    data_info = {
        "æ•°æ®è·¯å¾„": config['data_path'],
        "éªŒè¯é›†æ¯”ä¾‹": config['val_size'],
        "æµ‹è¯•é›†æ¯”ä¾‹": config['test_size'],
        "éšæœºç§å­": config['random_seed']
    }

    training_params = {
        "æ‰¹æ¬¡å¤§å°": config['batch_size'],
        "è®­ç»ƒè½®æ•°": config['epochs'],
        "å­¦ä¹ ç‡": config['learning_rate'],
        "éšè—ç»´åº¦": config['hidden_dim'],
        "è®¾å¤‡": config['device']
    }

    if config['focus_trait'] is not None:
        trait_name = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§'][config['focus_trait']]
        training_params["è®­ç»ƒæ¨¡å¼"] = f"å•ç»´åº¦è®­ç»ƒ - {trait_name}"

    training_logger.log_training_info(model_info, data_info, training_params)

    print("="*80)
    print("å¤§äº”äººæ ¼å›¾ç¥ç»ç½‘ç»œæ¨¡å‹")
    print(f"BERTåµŒå…¥: {'å¯ç”¨' if config['use_bert'] else 'ç¦ç”¨'}")
    print(f"è®¾å¤‡: {config['device']}")
    print(f"æ•°æ®è·¯å¾„: {config['data_path']}")
    if config['focus_trait'] is not None:
        trait_name = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§'][config['focus_trait']]
        print(f"å•ç»´åº¦è®­ç»ƒç»´åº¦: {trait_name}")

    if not os.path.exists(config['data_path']):
        print(f"é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ - {config['data_path']}")
        return

    try:
        dataset = BigFiveDataset(
            data_path=config['data_path'],
            use_bert=config['use_bert']
        )

        print("\næ­£åœ¨æ ¹æ®æ•°æ®é›†splitå­—æ®µè¿›è¡Œæ•°æ®åˆ’åˆ†...")

        train_indices = [i for i, item in enumerate(dataset.data) if item.get('split') == 'train']
        val_indices = [i for i, item in enumerate(dataset.data) if item.get('split') == 'valid']
        test_indices = [i for i, item in enumerate(dataset.data) if item.get('split') == 'test']

        print(f"æ•°æ®åˆ’åˆ†ç»Ÿè®¡ï¼š")
        print(f"  è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
        print(f"  æ€»è®¡: {len(train_indices) + len(val_indices) + len(test_indices)} æ ·æœ¬")

        if len(train_indices) + len(val_indices) + len(test_indices) != len(dataset):
            print(f"\nè­¦å‘Šï¼šæ•°æ®åˆ’åˆ†ä¸å®Œæ•´ï¼")
            print(f"  æ•°æ®é›†æ€»æ•°: {len(dataset)}")
            print(f"  å·²åˆ’åˆ†æ€»æ•°: {len(train_indices) + len(val_indices) + len(test_indices)}")

            all_split_values = set(item.get('split', 'unknown') for item in dataset.data)
            print(f"  æ•°æ®é›†ä¸­çš„splitå€¼: {all_split_values}")

        train_dataset = Subset(dataset, train_indices)
        val_dataset = Subset(dataset, val_indices)
        test_dataset = Subset(dataset, test_indices)

        if config.get('use_weighted_scene_sampler', False):
            weights = _compute_scene_weights_for_subset(
                dataset,
                train_indices,
                alpha=config.get('scene_weight_alpha', 0.5),
                w_min=config.get('scene_weight_clip_min', 1.0),
                w_max=config.get('scene_weight_clip_max', 3.0)
            )

            num_samples = int(len(train_indices) * config.get('samples_per_epoch_factor', 1.0))
            sampler = WeightedRandomSampler(weights=weights, num_samples=num_samples, replacement=True)

            train_loader = DataLoader(
                train_dataset,
                batch_size=config['batch_size'],
                shuffle=False,
                sampler=sampler,
                num_workers=8,
                pin_memory=True,
                persistent_workers=True,
                prefetch_factor=2
            )

            w_min_value = min(weights) if weights else 0.0
            w_max_value = max(weights) if weights else 0.0
            w_median = statistics.median(weights) if weights else 0.0
            w_mean = float(sum(weights) / len(weights)) if weights else 0.0
            print("\nå¯ç”¨åœºæ™¯é¢‘ç‡åŠ æƒé‡‡æ ·ï¼š")
            print(
                f"  alpha={config.get('scene_weight_alpha', 0.5)}, "
                f"clip=[{config.get('scene_weight_clip_min', 1.0)}, {config.get('scene_weight_clip_max', 3.0)}], "
                f"samples/epoch={num_samples}, replacement=True"
            )
            print(
                f"  æƒé‡ç»Ÿè®¡ -> min={w_min_value:.3f}, median={w_median:.3f}, "
                f"mean={w_mean:.3f}, max={w_max_value:.3f}"
            )
        else:
            train_loader = DataLoader(
                train_dataset,
                batch_size=config['batch_size'],
                shuffle=True,
                num_workers=8,
                pin_memory=True,
                persistent_workers=True,
                prefetch_factor=2
            )

        val_loader = DataLoader(
            val_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=8,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=config['batch_size'],
            shuffle=False,
            num_workers=8,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )

        input_dim = 768
        model = BigFiveGNN(
            input_dim=input_dim,
            hidden_dim=config['hidden_dim'],
            output_dim=15,
            num_layers=2,
            dropout=0.3
        ).to(config['device'])

        if os.path.exists(config['model_path']):
            print(f"\næ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶: {config['model_path']}ï¼Œè·³è¿‡è®­ç»ƒé˜¶æ®µï¼Œç›´æ¥åŠ è½½æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            trained_model = train_model(
                model=model,
                train_loader=train_loader,
                val_loader=val_loader,
                num_epochs=config['epochs'],
                learning_rate=config['learning_rate'],
                device=config['device'],
                save_path=config['model_path'],
                focus_trait=config['focus_trait']
            )

            print(f"\n" + "="*80)
            print("è®­ç»ƒå®Œæˆï¼")
            print(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {config['model_path']}")

        output_json_path = os.path.abspath(os.path.join(os.path.dirname(config['model_path']), 'prediction_visualization.json'))
        evaluate_on_test_set(config['model_path'], test_loader, config['device'], test_dataset, output_json_path, focus_trait=config['focus_trait'])

        final_metrics = {
            "è®­ç»ƒçŠ¶æ€": "å®Œæˆ",
            "æ¨¡å‹ä¿å­˜è·¯å¾„": config['model_path'],
            "è¯„ä¼°ç»“æœè·¯å¾„": output_json_path
        }

        training_logger.log_training_complete(final_metrics, 0)

    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

    finally:
        if 'training_logger' in locals():
            training_logger.restore_print()
            print(f"\nğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜åˆ°: {training_logger.log_path}")


if __name__ == "__main__":
    main()

"""
è®­ç»ƒæ¨¡å— - TrainingLogger, train_model, evaluate_on_test_set
"""

import os
import sys
import logging
import torch
import torch.nn as nn
import numpy as np
from datetime import datetime
from typing import Optional
from torch_geometric.loader import DataLoader

from .models import BigFiveGNN
from .losses import BigFiveLoss
from .utils import calculate_accuracy_detailed, calculate_evidence_f1_score

from .evaluator import ModelEvaluator


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""

    def __init__(self, log_dir="logs", log_filename=None):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨

        Args:
            log_dir: æ—¥å¿—ç›®å½•
            log_filename: æ—¥å¿—æ–‡ä»¶å
        """
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)

        if log_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_filename = f"training_{timestamp}.log"

        self.log_path = os.path.join(log_dir, log_filename)

        self.logger = logging.getLogger('TrainingLogger')
        self.logger.setLevel(logging.DEBUG)
        self.logger.handlers.clear()

        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)

        file_handler = logging.FileHandler(self.log_path, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)

        self.original_print = print

        self.logger.info(f"è®­ç»ƒæ—¥å¿—å·²åˆå§‹åŒ–ï¼Œæ—¥å¿—æ–‡ä»¶: {self.log_path}")
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: {self.log_path}")

    def log_print(self, *args, **kwargs):
        """æ›¿ä»£printå‡½æ•°ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—æ–‡ä»¶"""
        self.original_print(*args, **kwargs)
        message = ' '.join(str(arg) for arg in args)

        if any(keyword in message.lower() for keyword in ['error', 'é”™è¯¯', 'warning', 'è­¦å‘Š']):
            level = logging.WARNING
        elif any(keyword in message.lower() for keyword in ['epoch', 'loss', 'acc', 'f1', 'å‡†ç¡®', 'æŸå¤±']):
            level = logging.INFO
        else:
            level = logging.INFO

        self.logger.log(level, message)

    def log_training_info(self, model_info, data_info, training_params):
        """è®°å½•è®­ç»ƒåŸºæœ¬ä¿¡æ¯"""
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹è®­ç»ƒ - å¤§äº”äººæ ¼å›¾ç¥ç»ç½‘ç»œæ¨¡å‹")
        self.logger.info("=" * 80)

        self.logger.info("æ¨¡å‹é…ç½®:")
        for key, value in model_info.items():
            self.logger.info(f"  {key}: {value}")

        self.logger.info("æ•°æ®é›†ä¿¡æ¯:")
        for key, value in data_info.items():
            self.logger.info(f"  {key}: {value}")

        self.logger.info("è®­ç»ƒå‚æ•°:")
        for key, value in training_params.items():
            self.logger.info(f"  {key}: {value}")

        self.logger.info("=" * 80)

    def log_epoch_metrics(self, epoch, train_metrics, val_metrics, evidence_metrics=None):
        """è®°å½•æ¯ä¸ªepochçš„è¯¦ç»†æŒ‡æ ‡"""
        self.logger.info(f"Epoch {epoch + 1} è¯¦ç»†æŒ‡æ ‡:")

        self.logger.info("  è®­ç»ƒé›†:")
        for key, value in train_metrics.items():
            self.logger.info(f"    {key}: {value:.6f}")

        self.logger.info("  éªŒè¯é›†:")
        for key, value in val_metrics.items():
            self.logger.info(f"    {key}: {value:.6f}")

        if evidence_metrics:
            self.logger.info("  è¯æ®å¥:")
            for key, value in evidence_metrics.items():
                self.logger.info(f"    {key}: {value:.6f}")

    def log_model_save(self, save_path, metrics, is_best=False):
        """è®°å½•æ¨¡å‹ä¿å­˜ä¿¡æ¯"""
        status = "æœ€ä½³æ¨¡å‹" if is_best else "æ£€æŸ¥ç‚¹æ¨¡å‹"
        self.logger.info(f"ä¿å­˜{status}: {save_path}")
        self.logger.info(f"ä¿å­˜æ—¶æŒ‡æ ‡: {metrics}")

    def log_training_complete(self, final_metrics, total_time):
        """è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯"""
        self.logger.info("=" * 80)
        self.logger.info("è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f} ç§’")

        self.logger.info("æœ€ç»ˆæŒ‡æ ‡:")
        for key, value in final_metrics.items():
            if isinstance(value, (int, float)):
                self.logger.info(f"  {key}: {value:.6f}")
            else:
                self.logger.info(f"  {key}: {value}")

        self.logger.info("=" * 80)

    def replace_print(self):
        """æ›¿æ¢å…¨å±€printå‡½æ•°"""
        import builtins
        builtins.print = self.log_print

    def restore_print(self):
        """æ¢å¤åŸå§‹printå‡½æ•°"""
        import builtins
        builtins.print = self.original_print


def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,
                num_epochs: int = 50, learning_rate: float = 0.001,
                device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                save_path: str = 'best_bigfive_yuzhi.pth',
                focus_trait: Optional[int] = None):
    """
    è®­ç»ƒæ¨¡å‹

    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        device: è®¾å¤‡
        save_path: ä¿å­˜è·¯å¾„
        focus_trait: å…³æ³¨ç»´åº¦
    """
    model = model.to(device)
    if device == 'cuda':
        scaler = torch.amp.GradScaler('cuda')
    else:
        scaler = torch.amp.GradScaler('cpu')

    bert_params = list(model.bert_model.parameters())

    shared_params = []
    for gcn_block in model.gcn_blocks:
        shared_params.extend(list(gcn_block.parameters()))
    classifier_params = [list(h.parameters()) for h in model.personality_classifiers]
    flat_classifier_params = []
    for group in classifier_params:
        flat_classifier_params += group

    optimizer = torch.optim.AdamW([
        {'params': bert_params, 'lr': learning_rate, 'weight_decay': 0.01},
        {'params': shared_params, 'lr': 5e-4, 'weight_decay': 1e-4},
        {'params': flat_classifier_params, 'lr': 5e-4, 'weight_decay': 1e-4}
    ])

    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.8,
    )

    base_bert_lr = learning_rate
    base_nonbert_lr = 5e-4
    total_steps = max(1, len(train_loader) * num_epochs)
    warmup_steps = max(1, int(total_steps * 0.1))

    for i, g in enumerate(optimizer.param_groups):
        if i == 0:
            g['lr'] = base_bert_lr
        else:
            g['lr'] = 0.0

    criterion = BigFiveLoss(use_focal_loss=False, pos_weight=5.0, evidence_weight=1.0)
    print(f"ä½¿ç”¨æ­£æ ·æœ¬æƒé‡: {criterion.pos_weight} (å¯¹è¯æ®å¥åŠ æƒ{criterion.pos_weight}å€)")
    print(f"è¯æ®å¥æŸå¤±æƒé‡: {criterion.evidence_weight} (ä¼˜å…ˆä¼˜åŒ–è¯æ®å¥é¢„æµ‹)")

    evaluator = ModelEvaluator(num_classes=15, device=device)

    print("å¼€å§‹è®­ç»ƒ...")
    if focus_trait is not None:
        trait_name = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§'][focus_trait]
        print(f"å•ç»´åº¦æ¨¡å¼ï¼Œå½“å‰ç»´åº¦: {trait_name}")

    best_val_f1 = 0.0
    current_step = 0
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        all_train_logits = []
        all_train_labels = []
        train_evidence_metrics = []

        for batch_data in train_loader:
            batch_data = batch_data.to(device)
            optimizer.zero_grad()

            device_type = "cuda" if ("cuda" in str(device).lower()) else "cpu"

            with torch.autocast(device_type=device_type, enabled=(device_type == "cuda")):
                output, evidence_predictions_dict, evidence_predictions_raw = model(
                    batch_data.input_ids,
                    batch_data.attention_mask,
                    batch_data.personality_mask,
                    batch_data.adjacency_matrix,
                    batch_data.batch
                )

                if hasattr(batch_data, 'utter_ids'):
                    true_evidence = batch_data.utter_ids[0] if isinstance(batch_data.utter_ids, (list, tuple)) else batch_data.utter_ids

                    loss, loss_metrics = criterion(
                        output,
                        batch_data.labels.view_as(output),
                        evidence_predictions_raw=evidence_predictions_raw,
                        true_evidence=true_evidence,
                        focus_trait=focus_trait,
                        return_metrics=True
                    )

                    train_evidence_metrics.append(loss_metrics)
                else:
                    print("[è­¦å‘Š] å½“å‰æ‰¹æ¬¡ç¼ºå°‘è¯æ®å¥æ ‡ç­¾ï¼Œå›é€€åˆ°åŸæœ‰æŸå¤±è®¡ç®—æ–¹å¼ã€‚")

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()
            all_train_logits.append(output.detach())
            all_train_labels.append(batch_data.labels.view_as(output).detach())

            current_step += 1
            if current_step < warmup_steps:
                scale = float(current_step) / float(warmup_steps)
                for i, g in enumerate(optimizer.param_groups):
                    if i == 0:
                        g['lr'] = base_bert_lr
                    else:
                        g['lr'] = base_nonbert_lr * scale
            else:
                for i, g in enumerate(optimizer.param_groups):
                    if i == 0:
                        g['lr'] = base_bert_lr
                    else:
                        g['lr'] = base_nonbert_lr

        avg_train_loss = train_loss / len(train_loader)
        all_train_pred_tensor = torch.cat(all_train_logits, dim=0)
        all_train_label_tensor = torch.cat(all_train_labels, dim=0)

        if focus_trait is not None:
            train_acc = calculate_accuracy_detailed(all_train_pred_tensor, all_train_label_tensor, focus_trait)
        else:
            train_metrics = calculate_accuracy_detailed(all_train_pred_tensor, all_train_label_tensor)
            train_acc = train_metrics['overall_accuracy']

        model.eval()
        val_loss = 0.0
        all_logits = []
        all_labels = []

        val_evidence_f1_scores = []
        val_evidence_metrics = []
        val_evidence_predictions_list = []
        val_true_evidence_list = []
        val_evidence_detailed_metrics = []

        with torch.no_grad():
            for batch_data in val_loader:
                batch_data = batch_data.to(device)
                device_type = 'cuda' if str(device).startswith('cuda') else 'cpu'
                with torch.autocast(device_type=device_type, enabled=(device_type == 'cuda')):
                    output, evidence_predictions_dict, evidence_predictions_raw = model(
                        batch_data.input_ids,
                        batch_data.attention_mask,
                        batch_data.personality_mask,
                        batch_data.adjacency_matrix,
                        batch_data.batch
                    )

                    if hasattr(batch_data, 'utter_ids'):
                        true_evidence = batch_data.utter_ids[0] if isinstance(batch_data.utter_ids, (list, tuple)) else batch_data.utter_ids

                        loss, loss_metrics = criterion(
                            output,
                            batch_data.labels.view_as(output),
                            evidence_predictions_raw=evidence_predictions_raw,
                            true_evidence=true_evidence,
                            focus_trait=focus_trait,
                            return_metrics=True
                        )

                        val_evidence_metrics.append(loss_metrics)
                        val_evidence_predictions_list.append(evidence_predictions_dict)
                        val_true_evidence_list.append(true_evidence)
                    else:
                        print("[è­¦å‘Š] å½“å‰éªŒè¯æ‰¹æ¬¡ç¼ºå°‘è¯æ®å¥æ ‡ç­¾æˆ–ä¸æ˜¯å•ä¸ªå›¾ï¼Œå›é€€åˆ°åŸæœ‰æŸå¤±è®¡ç®—æ–¹å¼ã€‚")

                val_loss += loss.item()
                all_logits.append(output)
                all_labels.append(batch_data.labels.view_as(output).float())

        avg_val_loss = val_loss / len(val_loader)
        all_val_pred_tensor = torch.cat(all_logits, dim=0)
        all_val_label_tensor = torch.cat(all_labels, dim=0)

        if focus_trait is not None:
            val_acc = calculate_accuracy_detailed(all_val_pred_tensor, all_val_label_tensor, focus_trait)
        else:
            val_metrics = calculate_accuracy_detailed(all_val_pred_tensor, all_val_label_tensor)
            val_acc = val_metrics['overall_accuracy']

        if val_evidence_predictions_list and val_true_evidence_list:
            for pred_evidence, true_evidence in zip(val_evidence_predictions_list, val_true_evidence_list):
                evidence_f1_result = calculate_evidence_f1_score(true_evidence, pred_evidence)
                val_evidence_f1_scores.append(evidence_f1_result['avg_f1_score'])
                val_evidence_detailed_metrics.append(evidence_f1_result)

            avg_val_evidence_f1 = sum(val_evidence_f1_scores) / len(val_evidence_f1_scores) if val_evidence_f1_scores else 0.0
        else:
            avg_val_evidence_f1 = 0.0

        if val_evidence_metrics:
            evidence_losses = [m.get('evidence_loss', 0.0) for m in val_evidence_metrics if 'evidence_loss' in m]
            avg_val_evidence_loss = sum(evidence_losses) / len(evidence_losses) if evidence_losses else 0.0
        else:
            avg_val_evidence_loss = 0.0

        if all_logits:
            all_pred_tensor = torch.cat(all_logits, dim=0)
            all_label_tensor = torch.cat(all_labels, dim=0)
            val_metrics = evaluator.evaluate(all_pred_tensor, all_label_tensor, focus_trait=focus_trait)

            if focus_trait is not None:
                val_metrics = {
                    'trait_accuracy': val_metrics['overall_metrics']['trait_accuracy'],
                    'trait_f1': val_metrics['overall_metrics']['trait_f1']
                }
            else:
                val_metrics = {
                    'sample_accuracy': val_metrics['overall_metrics']['sample_accuracy'],
                    'fully_correct_accuracy': val_metrics['overall_metrics']['fully_correct_accuracy'],
                    'avg_trait_accuracy': val_metrics['overall_metrics']['avg_trait_accuracy'],
                    'avg_trait_f1': val_metrics['overall_metrics']['avg_trait_f1'],
                    'trait_accuracies': [val_metrics['trait_metrics'][f'trait_{i}']['accuracy'] for i in range(5)],
                    'trait_f1_scores': [val_metrics['trait_metrics'][f'trait_{i}']['f1_score'] for i in range(5)]
                }
        else:
            print("[è­¦å‘Š] å½“å‰éªŒè¯é›†ä¸­æ²¡æœ‰é¢„æµ‹ç»“æœï¼Œæ— æ³•è®¡ç®—è¯¦ç»†æŒ‡æ ‡ã€‚")

        scheduler.step(avg_val_loss)

        print(f"Epoch {epoch+1:2d}:")
        print(f"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}")

        if focus_trait is not None:
            print(f"  Trait Accuracy: {val_metrics['trait_accuracy']:.4f}")
            print(f"  Trait F1: {val_metrics['trait_f1']:.4f}")
        else:
            print(f"  Sample Accuracy: {val_metrics['sample_accuracy']:.4f}")
            print(f"  Fully Correct Accuracy: {val_metrics['fully_correct_accuracy']:.4f}")
            print(f"  Average Trait Accuracy: {val_metrics['avg_trait_accuracy']:.4f}")
            print(f"  Average Trait F1: {val_metrics['avg_trait_f1']:.4f}")

        if val_evidence_f1_scores:
            print(f"  è¯æ®å¥å¹³å‡F1åˆ†æ•°: {avg_val_evidence_f1:.4f}")

            if val_evidence_detailed_metrics:
                personality_dims = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
                trait_names_cn = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§']

                dim_avg_metrics = {}
                for dim in personality_dims:
                    dim_weighted_f1_list = [m['dimension_metrics'][dim]['weighted_f1'] for m in val_evidence_detailed_metrics]
                    dim_acc_list = [m['dimension_metrics'][dim]['accuracy'] for m in val_evidence_detailed_metrics]

                    dim_avg_metrics[dim] = {
                        'weighted_f1': np.mean(dim_weighted_f1_list),
                        'accuracy': np.mean(dim_acc_list)
                    }

                overall_weighted_f1_list = [m['overall_weighted_f1'] for m in val_evidence_detailed_metrics]
                overall_accuracy_list = [m['overall_accuracy'] for m in val_evidence_detailed_metrics]

                overall_avg_weighted_f1 = np.mean(overall_weighted_f1_list)
                overall_avg_accuracy = np.mean(overall_accuracy_list)

                print(f"    æ€»ä½“ Weighted F1: {overall_avg_weighted_f1:.4f}, æ€»ä½“ Acc: {overall_avg_accuracy:.4f}")
                print(f"    ç»´åº¦æŒ‡æ ‡ - ", end="")
                for en_name, cn_name in zip(personality_dims, trait_names_cn):
                    metrics = dim_avg_metrics[en_name]
                    print(f"{cn_name}:{metrics['weighted_f1']:.3f}/{metrics['accuracy']:.3f} ", end="")
                print()

        if val_evidence_metrics:
            print(f"  è¯æ®å¥æŸå¤±: {avg_val_evidence_loss:.4f}")

        trait_names = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§']
        for i, trait_name in enumerate(trait_names):
            print(f"  {trait_name} - Accuracy: {val_metrics['trait_accuracies'][i]:.4f}, F1: {val_metrics['trait_f1_scores'][i]:.4f}")

        if avg_val_evidence_f1 > best_val_f1:
            best_val_f1 = avg_val_evidence_f1
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_val_loss,
                'metrics': val_metrics,
                'config': {
                    'input_dim': model.input_dim,
                    'hidden_dim': model.hidden_dim,
                    'output_dim': model.output_dim,
                    'num_layers': model.num_layers
                }
            }, save_path)

            if focus_trait is not None:
                print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (Trait Accuracy")
            else:
                print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (Fully Evidence F1: {best_val_f1:.4f})")

    print("è®­ç»ƒå®Œæˆï¼")
    return model


def evaluate_on_test_set(model_path, test_loader, device, test_dataset, output_json_path, focus_trait: Optional[int] = None):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        test_dataset: æµ‹è¯•æ•°æ®é›†
        output_json_path: è¾“å‡ºJSONè·¯å¾„
        focus_trait: å…³æ³¨çš„äººæ ¼ç»´åº¦
    """
    print("\n" + "="*80)
    print("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹...")

    if not os.path.exists(model_path):
        print(f"é”™è¯¯:æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ - {model_path}")
        return

    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    model_config = checkpoint['config']

    model = BigFiveGNN(
        input_dim=model_config['input_dim'],
        hidden_dim=model_config['hidden_dim'],
        output_dim=model_config['output_dim'],
        num_layers=model_config['num_layers']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    evaluator = ModelEvaluator(num_classes=15, device=device, output_json_path=output_json_path)

    all_predictions = []
    all_labels = []
    all_dialogue_ids = []
    all_original_texts = []
    all_character_names = []

    test_evidence_f1_scores = []
    test_evidence_predictions_list = []
    test_true_evidence_list = []
    test_evidence_detailed_metrics = []

    with torch.no_grad():
        for batch_data in test_loader:
            batch_data = batch_data.to(device)
            device_type = 'cuda' if str(device).startswith('cuda') else 'cpu'
            with torch.autocast(device_type=device_type, enabled=(device_type == 'cuda')):
                output, evidence_predictions_dict, evidence_predictions_raw = model(
                    batch_data.input_ids,
                    batch_data.attention_mask,
                    batch_data.personality_mask,
                    batch_data.adjacency_matrix,
                    batch_data.batch
                )

            all_predictions.append(output)
            all_labels.append(batch_data.labels.view_as(output))
            all_dialogue_ids.extend(batch_data.dialogue_id)
            all_original_texts.extend(batch_data.dialogue)
            all_character_names.extend(batch_data.target_character)

            if hasattr(batch_data, 'utter_ids'):
                true_evidence = batch_data.utter_ids[0] if isinstance(batch_data.utter_ids, (list, tuple)) else batch_data.utter_ids
                evidence_f1_result = calculate_evidence_f1_score(true_evidence, evidence_predictions_dict)

                test_evidence_f1_scores.append(evidence_f1_result['avg_f1_score'])
                test_evidence_detailed_metrics.append(evidence_f1_result)

                evidence_pred_serializable = {}
                for dim_name, pred_tensor in evidence_predictions_dict.items():
                    evidence_pred_serializable[dim_name] = pred_tensor.cpu().tolist()
                test_evidence_predictions_list.append(evidence_pred_serializable)

                personality_dims = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
                num_utterances = evidence_predictions_dict['openness'].size(0)

                true_evidence_serializable = {}
                for dim_name in personality_dims:
                    true_vector = [0] * num_utterances
                    evidence_indices = true_evidence.get(dim_name, [])

                    for idx_str in evidence_indices:
                        if not idx_str or idx_str.strip() == '':
                            continue

                        for idx_part in idx_str.split(','):
                            try:
                                idx_part = idx_part.strip()
                                if idx_part:
                                    idx_int = int(idx_part) - 1
                                    if 0 <= idx_int < num_utterances:
                                        true_vector[idx_int] = 1
                            except (ValueError, TypeError):
                                continue

                    true_evidence_serializable[dim_name] = true_vector

                test_true_evidence_list.append(true_evidence_serializable)

    if not all_predictions:
        print("æµ‹è¯•é›†ä¸ºç©º,æ— æ³•è¯„ä¼°ã€‚")
        return

    all_pred_tensor = torch.cat(all_predictions, dim=0)
    all_label_tensor = torch.cat(all_labels, dim=0)

    test_metrics = evaluator.evaluate(
        all_pred_tensor,
        all_label_tensor,
        all_dialogue_ids,
        all_original_texts,
        all_character_names,
        focus_trait=focus_trait,
        evidence_f1_scores=test_evidence_f1_scores if test_evidence_f1_scores else None,
        evidence_predictions=test_evidence_predictions_list if test_evidence_predictions_list else None,
        true_evidence_labels=test_true_evidence_list if test_true_evidence_list else None
    )

    print("\næµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    if focus_trait is not None:
        trait_name = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§'][focus_trait]
        print(f" å½“å‰ç»´åº¦: {trait_name}")
        print(f"  Trait Accuracy: {test_metrics['overall_metrics']['trait_accuracy']:.4f}")
        print(f"  Trait F1: {test_metrics['overall_metrics']['trait_f1']:.4f}")
    else:
        print(f"  Sample Accuracy: {test_metrics['overall_metrics']['sample_accuracy']:.4f}")
        print(f"  Fully Correct Accuracy: {test_metrics['overall_metrics']['fully_correct_accuracy']:.4f}")
        print(f"  Average Trait Accuracy: {test_metrics['overall_metrics']['avg_trait_accuracy']:.4f}")
        print(f"  Average Trait F1: {test_metrics['overall_metrics']['avg_trait_f1']:.4f}")

        trait_names = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§']
        print("\n  æ¯ä¸ªç‰¹è´¨çš„å‡†ç¡®ç‡å’ŒF1åˆ†æ•°:")
        for i, trait_name in enumerate(trait_names):
            print(f"    {trait_name} - Accuracy: {test_metrics['trait_metrics'][f'trait_{i}']['accuracy']:.4f}, F1: {test_metrics['trait_metrics'][f'trait_{i}']['f1_score']:.4f}")

    if test_evidence_f1_scores:
        avg_evidence_f1 = test_metrics['overall_metrics'].get('avg_evidence_f1', 0.0)
        print(f"\n  è¯æ®å¥å¹³å‡F1åˆ†æ•°: {avg_evidence_f1:.4f}")

        if test_evidence_detailed_metrics:
            personality_dims = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
            trait_names_cn = ['å¼€æ”¾æ€§', 'å°½è´£æ€§', 'å¤–å‘æ€§', 'å®œäººæ€§', 'ç¥ç»è´¨æ€§']

            dim_avg_metrics = {}
            for dim in personality_dims:
                dim_f1_list = [m['dimension_metrics'][dim]['f1'] for m in test_evidence_detailed_metrics]
                dim_weighted_f1_list = [m['dimension_metrics'][dim]['weighted_f1'] for m in test_evidence_detailed_metrics]
                dim_acc_list = [m['dimension_metrics'][dim]['accuracy'] for m in test_evidence_detailed_metrics]

                dim_avg_metrics[dim] = {
                    'f1': np.mean(dim_f1_list),
                    'weighted_f1': np.mean(dim_weighted_f1_list),
                    'accuracy': np.mean(dim_acc_list)
                }

            overall_weighted_f1_list = [m['overall_weighted_f1'] for m in test_evidence_detailed_metrics]
            overall_accuracy_list = [m['overall_accuracy'] for m in test_evidence_detailed_metrics]
            avg_weighted_f1_list = [m['avg_weighted_f1'] for m in test_evidence_detailed_metrics]
            avg_accuracy_list = [m['avg_accuracy'] for m in test_evidence_detailed_metrics]

            overall_avg_weighted_f1 = np.mean(overall_weighted_f1_list)
            overall_avg_accuracy = np.mean(overall_accuracy_list)
            dim_avg_weighted_f1 = np.mean(avg_weighted_f1_list)
            dim_avg_accuracy = np.mean(avg_accuracy_list)

            print(f"\n  ã€è¯æ®å¥è¯¦ç»†æŒ‡æ ‡ã€‘")
            print(f"  æ€»ä½“ Weighted F1: {overall_avg_weighted_f1:.4f}")
            print(f"  æ€»ä½“ Accuracy: {overall_avg_accuracy:.4f}")
            print(f"  ç»´åº¦å¹³å‡ Weighted F1: {dim_avg_weighted_f1:.4f}")
            print(f"  ç»´åº¦å¹³å‡ Accuracy: {dim_avg_accuracy:.4f}")

            print("\n  å„ç»´åº¦è¯æ®å¥æŒ‡æ ‡:")
            for en_name, cn_name in zip(personality_dims, trait_names_cn):
                metrics = dim_avg_metrics[en_name]
                print(f"    {cn_name}: Acc={metrics['accuracy']:.4f}, "
                      f"F1={metrics['f1']:.4f}, "
                      f"Weighted-F1={metrics['weighted_f1']:.4f}")

    print("="*80)
